{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dc3a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4845c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef6fc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the string to a list of integers or floats\n",
    "def convert_to_list(string):\n",
    "    # Remove the brackets\n",
    "    string = string.strip(\"[]\")\n",
    "    \n",
    "    # Convert the string to a list of floats or ints\n",
    "    if '.' in string:  # Check if the numbers are floats\n",
    "        return list(map(float, string.split()))\n",
    "    else:\n",
    "        return list(map(int, string.split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42059114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Average F1 Score: 0.2143597283066372\n",
      "Bootstrap F1 Score Standard Deviation: 0.01187191089834755\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "sample_df = pd.read_csv('full_output_frame_zero_shot_high_res.csv')\n",
    "\n",
    "# Convert string representations of lists back to actual lists\n",
    "sample_df['parsed_answer'] = sample_df['parsed_answer'].apply(lambda x: convert_to_list(x))\n",
    "sample_df['ground_truth'] = sample_df['ground_truth'].apply(lambda x: convert_to_list(x))\n",
    "\n",
    "y_true = sample_df[\"ground_truth\"].tolist()\n",
    "y_pred = sample_df[\"parsed_answer\"].tolist()\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstrap_samples = 1000\n",
    "\n",
    "# Store the F1 scores\n",
    "f1_scores = []\n",
    "\n",
    "# Perform bootstrapping\n",
    "for _ in range(n_bootstrap_samples):\n",
    "    # Resample with replacement\n",
    "    y_true_resampled, y_pred_resampled = resample(y_true, y_pred, replace=True)\n",
    "    \n",
    "    # Calculate the F1 score\n",
    "    f1 = f1_score(y_true_resampled, y_pred_resampled, average='macro')\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate the average F1 score over all bootstrapped samples\n",
    "bootstrap_avg_f1 = np.mean(f1_scores)\n",
    "bootstrap_std_f1 = np.std(f1_scores)\n",
    "\n",
    "print(f'Bootstrap Average F1 Score: {bootstrap_avg_f1}')\n",
    "print(f'Bootstrap F1 Score Standard Deviation: {bootstrap_std_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8f69bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White F1:  0.21493140982288267\n",
      "Black F1:  0.21195375845242478\n"
     ]
    }
   ],
   "source": [
    "y_true_white = sample_df[sample_df.race=='White'][\"ground_truth\"].tolist()\n",
    "y_pred_white = sample_df[sample_df.race=='White'][\"parsed_answer\"].tolist()\n",
    "\n",
    "y_true_black = sample_df[sample_df.race=='Black'][\"ground_truth\"].tolist()\n",
    "y_pred_black = sample_df[sample_df.race=='Black'][\"parsed_answer\"].tolist()\n",
    "\n",
    "print('White F1: ',f1_score(y_true_white, y_pred_white, average='macro'))\n",
    "print('Black F1: ',f1_score(y_true_black, y_pred_black, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a68be13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male F1:  0.2264418581178549\n",
      "Female F1:  0.196686408700425\n"
     ]
    }
   ],
   "source": [
    "y_true_white = sample_df[sample_df.sex=='Male'][\"ground_truth\"].tolist()\n",
    "y_pred_white = sample_df[sample_df.sex=='Male'][\"parsed_answer\"].tolist()\n",
    "\n",
    "y_true_black = sample_df[sample_df.sex=='Female'][\"ground_truth\"].tolist()\n",
    "y_pred_black = sample_df[sample_df.sex=='Female'][\"parsed_answer\"].tolist()\n",
    "\n",
    "print('Male F1: ',f1_score(y_true_white, y_pred_white, average='macro'))\n",
    "print('Female F1: ',f1_score(y_true_black, y_pred_black, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a6fe15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random F1:  0.22004752111796164\n"
     ]
    }
   ],
   "source": [
    "n_samples = 499\n",
    "n_labels = 14\n",
    "\n",
    "# Random predictions for each label\n",
    "y_pred_random = np.random.randint(0, 2, size=(n_samples, n_labels))\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_random = f1_score(y_true, y_pred_random, average='macro')\n",
    "print('Random F1: ', f1_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37d8ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (biasenv)",
   "language": "python",
   "name": "biasenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
